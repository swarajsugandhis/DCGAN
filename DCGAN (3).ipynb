{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "us-7tENhNp4Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2BPDJsfNp4c",
        "outputId": "cad108f5-db00-4d49-9e5a-aca4b5833874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wND5L0oxNp4d"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "image_size = 64\n",
        "nz = 100  # Size of latent vector\n",
        "ngf = 64  # Generator feature maps\n",
        "ndf = 64  # Discriminator feature maps\n",
        "epochs = 10\n",
        "lr = 0.0002\n",
        "beta1 = 0.5  # Adam optimizer beta1 value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak1mgcMbNp4d"
      },
      "outputs": [],
      "source": [
        "dataroot = \"./Dataset_DCGANS\"\n",
        "dataset = dset.ImageFolder(root=dataroot, transform=transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "]))\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpXSSX5rNp4d"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf, 3, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "generator = Generator().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKTKBMtKNp4d"
      },
      "outputs": [],
      "source": [
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "discriminator = Discriminator().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCkZxJIWNp4e"
      },
      "outputs": [],
      "source": [
        "# Loss and Optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerD = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2VAk6rlNp4e",
        "outputId": "886737c6-ff1e-4b0e-b4d1-dc8910c4772c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0/10] Batch 0/1583 Loss D: 1.3705850839614868, Loss G: 2.437617301940918\n",
            "Epoch [0/10] Batch 100/1583 Loss D: 0.07309041917324066, Loss G: 6.464035987854004\n",
            "Epoch [0/10] Batch 200/1583 Loss D: 0.9527121186256409, Loss G: 5.781160354614258\n",
            "Epoch [0/10] Batch 300/1583 Loss D: 1.016509771347046, Loss G: 1.3346107006072998\n",
            "Epoch [0/10] Batch 400/1583 Loss D: 1.6353894472122192, Loss G: 7.176621437072754\n",
            "Epoch [0/10] Batch 500/1583 Loss D: 0.5331792831420898, Loss G: 2.8078701496124268\n",
            "Epoch [0/10] Batch 600/1583 Loss D: 0.3280215859413147, Loss G: 3.406755208969116\n",
            "Epoch [0/10] Batch 700/1583 Loss D: 0.7408376336097717, Loss G: 4.491232872009277\n",
            "Epoch [0/10] Batch 800/1583 Loss D: 0.6432062387466431, Loss G: 3.3877859115600586\n",
            "Epoch [0/10] Batch 900/1583 Loss D: 1.204240322113037, Loss G: 1.796863317489624\n",
            "Epoch [0/10] Batch 1000/1583 Loss D: 0.6074163913726807, Loss G: 3.6432042121887207\n",
            "Epoch [0/10] Batch 1100/1583 Loss D: 0.9701594114303589, Loss G: 2.1877872943878174\n",
            "Epoch [0/10] Batch 1200/1583 Loss D: 0.5221277475357056, Loss G: 3.1256980895996094\n",
            "Epoch [0/10] Batch 1300/1583 Loss D: 0.6442961096763611, Loss G: 2.8525209426879883\n",
            "Epoch [0/10] Batch 1400/1583 Loss D: 0.6810413002967834, Loss G: 4.3269195556640625\n",
            "Epoch [0/10] Batch 1500/1583 Loss D: 0.32327044010162354, Loss G: 3.5363664627075195\n",
            "Epoch [1/10] Batch 0/1583 Loss D: 2.214509963989258, Loss G: 1.3130898475646973\n",
            "Epoch [1/10] Batch 100/1583 Loss D: 0.4894085228443146, Loss G: 3.243586301803589\n",
            "Epoch [1/10] Batch 200/1583 Loss D: 0.759713888168335, Loss G: 3.7123751640319824\n",
            "Epoch [1/10] Batch 300/1583 Loss D: 0.7760818600654602, Loss G: 2.1070797443389893\n",
            "Epoch [1/10] Batch 400/1583 Loss D: 0.5522440671920776, Loss G: 2.1793782711029053\n",
            "Epoch [1/10] Batch 500/1583 Loss D: 1.2127026319503784, Loss G: 2.862121343612671\n",
            "Epoch [1/10] Batch 600/1583 Loss D: 0.47812506556510925, Loss G: 4.111968517303467\n",
            "Epoch [1/10] Batch 700/1583 Loss D: 0.39084625244140625, Loss G: 3.0803780555725098\n",
            "Epoch [1/10] Batch 800/1583 Loss D: 0.9829097390174866, Loss G: 6.274323463439941\n",
            "Epoch [1/10] Batch 900/1583 Loss D: 0.6477330327033997, Loss G: 1.517174243927002\n",
            "Epoch [1/10] Batch 1000/1583 Loss D: 0.46709510684013367, Loss G: 4.152665138244629\n",
            "Epoch [1/10] Batch 1100/1583 Loss D: 0.49919623136520386, Loss G: 2.1350393295288086\n",
            "Epoch [1/10] Batch 1200/1583 Loss D: 0.9849480390548706, Loss G: 1.4696440696716309\n",
            "Epoch [1/10] Batch 1300/1583 Loss D: 0.7666935920715332, Loss G: 1.8547433614730835\n",
            "Epoch [1/10] Batch 1400/1583 Loss D: 0.43539735674858093, Loss G: 2.6319212913513184\n",
            "Epoch [1/10] Batch 1500/1583 Loss D: 0.3914496600627899, Loss G: 2.8265151977539062\n",
            "Epoch [2/10] Batch 0/1583 Loss D: 0.7396180033683777, Loss G: 5.323039531707764\n",
            "Epoch [2/10] Batch 100/1583 Loss D: 0.3271593749523163, Loss G: 4.228822708129883\n",
            "Epoch [2/10] Batch 200/1583 Loss D: 0.4896138608455658, Loss G: 2.511704921722412\n",
            "Epoch [2/10] Batch 300/1583 Loss D: 0.6119375228881836, Loss G: 3.140615463256836\n",
            "Epoch [2/10] Batch 400/1583 Loss D: 1.6335723400115967, Loss G: 0.9488776922225952\n",
            "Epoch [2/10] Batch 500/1583 Loss D: 0.6868743896484375, Loss G: 5.230230331420898\n",
            "Epoch [2/10] Batch 600/1583 Loss D: 0.3178848922252655, Loss G: 3.4731099605560303\n",
            "Epoch [2/10] Batch 700/1583 Loss D: 0.3149264454841614, Loss G: 3.0975630283355713\n",
            "Epoch [2/10] Batch 800/1583 Loss D: 0.6951665282249451, Loss G: 4.906651973724365\n",
            "Epoch [2/10] Batch 900/1583 Loss D: 0.30620062351226807, Loss G: 2.7626543045043945\n",
            "Epoch [2/10] Batch 1000/1583 Loss D: 0.40094029903411865, Loss G: 1.789673089981079\n",
            "Epoch [2/10] Batch 1100/1583 Loss D: 0.4751948118209839, Loss G: 3.243779182434082\n",
            "Epoch [2/10] Batch 1200/1583 Loss D: 0.2372153103351593, Loss G: 3.224376678466797\n",
            "Epoch [2/10] Batch 1300/1583 Loss D: 0.3111444115638733, Loss G: 4.341442584991455\n",
            "Epoch [2/10] Batch 1400/1583 Loss D: 0.44394463300704956, Loss G: 3.9940335750579834\n",
            "Epoch [2/10] Batch 1500/1583 Loss D: 0.3285383880138397, Loss G: 2.7679076194763184\n",
            "Epoch [3/10] Batch 0/1583 Loss D: 0.3386188745498657, Loss G: 3.308182716369629\n",
            "Epoch [3/10] Batch 100/1583 Loss D: 0.6248672008514404, Loss G: 2.3923521041870117\n",
            "Epoch [3/10] Batch 200/1583 Loss D: 1.5236024856567383, Loss G: 0.6310957670211792\n",
            "Epoch [3/10] Batch 300/1583 Loss D: 0.5470138192176819, Loss G: 5.538322448730469\n",
            "Epoch [3/10] Batch 400/1583 Loss D: 0.32788702845573425, Loss G: 3.3459033966064453\n",
            "Epoch [3/10] Batch 500/1583 Loss D: 0.25809329748153687, Loss G: 4.140803813934326\n",
            "Epoch [3/10] Batch 600/1583 Loss D: 3.1995644569396973, Loss G: 5.811518669128418\n",
            "Epoch [3/10] Batch 700/1583 Loss D: 0.36741581559181213, Loss G: 3.1662983894348145\n",
            "Epoch [3/10] Batch 800/1583 Loss D: 0.21685771644115448, Loss G: 3.108576774597168\n",
            "Epoch [3/10] Batch 900/1583 Loss D: 0.17741328477859497, Loss G: 3.095118999481201\n",
            "Epoch [3/10] Batch 1000/1583 Loss D: 0.4524626135826111, Loss G: 7.306107521057129\n",
            "Epoch [3/10] Batch 1100/1583 Loss D: 0.46939778327941895, Loss G: 1.6424760818481445\n",
            "Epoch [3/10] Batch 1200/1583 Loss D: 0.2828333377838135, Loss G: 4.073389053344727\n",
            "Epoch [3/10] Batch 1300/1583 Loss D: 0.1931782364845276, Loss G: 3.6739468574523926\n",
            "Epoch [3/10] Batch 1400/1583 Loss D: 1.4360634088516235, Loss G: 8.296862602233887\n",
            "Epoch [3/10] Batch 1500/1583 Loss D: 0.26669400930404663, Loss G: 2.9239468574523926\n",
            "Epoch [4/10] Batch 0/1583 Loss D: 0.516262948513031, Loss G: 5.1684794425964355\n",
            "Epoch [4/10] Batch 100/1583 Loss D: 1.0364576578140259, Loss G: 1.7777118682861328\n",
            "Epoch [4/10] Batch 200/1583 Loss D: 0.7268503308296204, Loss G: 1.9523518085479736\n",
            "Epoch [4/10] Batch 300/1583 Loss D: 0.25346115231513977, Loss G: 2.6061418056488037\n",
            "Epoch [4/10] Batch 400/1583 Loss D: 0.300006628036499, Loss G: 4.055732250213623\n",
            "Epoch [4/10] Batch 500/1583 Loss D: 0.40310484170913696, Loss G: 4.271810531616211\n",
            "Epoch [4/10] Batch 600/1583 Loss D: 0.1658647358417511, Loss G: 3.9294068813323975\n",
            "Epoch [4/10] Batch 700/1583 Loss D: 0.3758953809738159, Loss G: 3.4299089908599854\n",
            "Epoch [4/10] Batch 800/1583 Loss D: 0.4245589077472687, Loss G: 4.178484916687012\n",
            "Epoch [4/10] Batch 900/1583 Loss D: 0.4488930404186249, Loss G: 1.3934588432312012\n",
            "Epoch [4/10] Batch 1000/1583 Loss D: 0.4377531111240387, Loss G: 5.613436698913574\n",
            "Epoch [4/10] Batch 1100/1583 Loss D: 0.914522111415863, Loss G: 1.0389387607574463\n",
            "Epoch [4/10] Batch 1200/1583 Loss D: 0.31289535760879517, Loss G: 5.04298734664917\n",
            "Epoch [4/10] Batch 1300/1583 Loss D: 0.2516523599624634, Loss G: 3.6079607009887695\n",
            "Epoch [4/10] Batch 1400/1583 Loss D: 0.6526869535446167, Loss G: 3.9757704734802246\n",
            "Epoch [4/10] Batch 1500/1583 Loss D: 0.4661409258842468, Loss G: 1.3415307998657227\n",
            "Epoch [5/10] Batch 0/1583 Loss D: 0.7486292719841003, Loss G: 6.248763561248779\n",
            "Epoch [5/10] Batch 100/1583 Loss D: 0.28507497906684875, Loss G: 3.5236153602600098\n",
            "Epoch [5/10] Batch 200/1583 Loss D: 0.14030301570892334, Loss G: 3.206149101257324\n",
            "Epoch [5/10] Batch 300/1583 Loss D: 0.41828134655952454, Loss G: 4.883179664611816\n",
            "Epoch [5/10] Batch 400/1583 Loss D: 0.4038560390472412, Loss G: 5.822271347045898\n",
            "Epoch [5/10] Batch 500/1583 Loss D: 0.5445250868797302, Loss G: 1.4323673248291016\n",
            "Epoch [5/10] Batch 600/1583 Loss D: 0.21112549304962158, Loss G: 3.455200672149658\n",
            "Epoch [5/10] Batch 700/1583 Loss D: 0.11506800353527069, Loss G: 4.656185150146484\n",
            "Epoch [5/10] Batch 800/1583 Loss D: 0.13859692215919495, Loss G: 3.5148138999938965\n",
            "Epoch [5/10] Batch 900/1583 Loss D: 0.32361486554145813, Loss G: 3.319854259490967\n",
            "Epoch [5/10] Batch 1000/1583 Loss D: 0.3533429205417633, Loss G: 4.179091453552246\n",
            "Epoch [5/10] Batch 1100/1583 Loss D: 0.14343488216400146, Loss G: 3.952342987060547\n",
            "Epoch [5/10] Batch 1200/1583 Loss D: 0.24691379070281982, Loss G: 4.423646450042725\n",
            "Epoch [5/10] Batch 1300/1583 Loss D: 0.3485219180583954, Loss G: 3.837747812271118\n",
            "Epoch [5/10] Batch 1400/1583 Loss D: 0.21005792915821075, Loss G: 3.9434704780578613\n",
            "Epoch [5/10] Batch 1500/1583 Loss D: 0.22324617207050323, Loss G: 2.553957462310791\n",
            "Epoch [6/10] Batch 0/1583 Loss D: 0.20969843864440918, Loss G: 3.960397720336914\n",
            "Epoch [6/10] Batch 100/1583 Loss D: 0.31751805543899536, Loss G: 2.9054718017578125\n",
            "Epoch [6/10] Batch 200/1583 Loss D: 0.1370750367641449, Loss G: 3.89359712600708\n",
            "Epoch [6/10] Batch 300/1583 Loss D: 0.3640102744102478, Loss G: 2.139399528503418\n",
            "Epoch [6/10] Batch 400/1583 Loss D: 0.273616760969162, Loss G: 4.612627029418945\n",
            "Epoch [6/10] Batch 500/1583 Loss D: 0.82326740026474, Loss G: 1.7964458465576172\n",
            "Epoch [6/10] Batch 600/1583 Loss D: 0.30843690037727356, Loss G: 4.523690223693848\n",
            "Epoch [6/10] Batch 700/1583 Loss D: 0.5032049417495728, Loss G: 3.3841781616210938\n",
            "Epoch [6/10] Batch 800/1583 Loss D: 0.21284453570842743, Loss G: 3.217639446258545\n",
            "Epoch [6/10] Batch 900/1583 Loss D: 0.5754268169403076, Loss G: 2.7472333908081055\n",
            "Epoch [6/10] Batch 1000/1583 Loss D: 0.23683521151542664, Loss G: 2.3477492332458496\n",
            "Epoch [6/10] Batch 1100/1583 Loss D: 0.593803346157074, Loss G: 3.828191041946411\n",
            "Epoch [6/10] Batch 1200/1583 Loss D: 0.3593546748161316, Loss G: 3.280745267868042\n",
            "Epoch [6/10] Batch 1300/1583 Loss D: 0.22736775875091553, Loss G: 2.576799154281616\n",
            "Epoch [6/10] Batch 1400/1583 Loss D: 0.14876089990139008, Loss G: 3.4051437377929688\n",
            "Epoch [6/10] Batch 1500/1583 Loss D: 0.32018008828163147, Loss G: 5.18256950378418\n",
            "Epoch [7/10] Batch 0/1583 Loss D: 0.185469388961792, Loss G: 2.6587631702423096\n",
            "Epoch [7/10] Batch 100/1583 Loss D: 1.0744932889938354, Loss G: 0.7506648302078247\n",
            "Epoch [7/10] Batch 200/1583 Loss D: 0.25283119082450867, Loss G: 4.382200241088867\n",
            "Epoch [7/10] Batch 300/1583 Loss D: 0.5235695838928223, Loss G: 2.7781381607055664\n",
            "Epoch [7/10] Batch 400/1583 Loss D: 0.13591396808624268, Loss G: 3.6185858249664307\n",
            "Epoch [7/10] Batch 500/1583 Loss D: 3.675017833709717, Loss G: 7.28513240814209\n",
            "Epoch [7/10] Batch 600/1583 Loss D: 0.17037168145179749, Loss G: 4.212945938110352\n",
            "Epoch [7/10] Batch 700/1583 Loss D: 0.30353814363479614, Loss G: 2.26688289642334\n",
            "Epoch [7/10] Batch 800/1583 Loss D: 0.12318705767393112, Loss G: 4.417876720428467\n",
            "Epoch [7/10] Batch 900/1583 Loss D: 0.15730008482933044, Loss G: 3.8255271911621094\n",
            "Epoch [7/10] Batch 1000/1583 Loss D: 0.10574135929346085, Loss G: 4.445579528808594\n",
            "Epoch [7/10] Batch 1100/1583 Loss D: 0.9164000153541565, Loss G: 1.2760380506515503\n",
            "Epoch [7/10] Batch 1200/1583 Loss D: 0.18369683623313904, Loss G: 3.905168294906616\n",
            "Epoch [7/10] Batch 1300/1583 Loss D: 0.1346672922372818, Loss G: 4.159313201904297\n",
            "Epoch [7/10] Batch 1400/1583 Loss D: 0.4975418150424957, Loss G: 4.5402021408081055\n",
            "Epoch [7/10] Batch 1500/1583 Loss D: 0.15731090307235718, Loss G: 3.610118865966797\n",
            "Epoch [8/10] Batch 0/1583 Loss D: 0.21207520365715027, Loss G: 2.83463978767395\n",
            "Epoch [8/10] Batch 100/1583 Loss D: 0.11752256751060486, Loss G: 3.764400005340576\n",
            "Epoch [8/10] Batch 200/1583 Loss D: 0.17124256491661072, Loss G: 4.397198677062988\n",
            "Epoch [8/10] Batch 300/1583 Loss D: 0.6971400380134583, Loss G: 2.767641544342041\n",
            "Epoch [8/10] Batch 400/1583 Loss D: 0.12892714142799377, Loss G: 3.6187620162963867\n",
            "Epoch [8/10] Batch 500/1583 Loss D: 0.16884657740592957, Loss G: 4.318778038024902\n",
            "Epoch [8/10] Batch 600/1583 Loss D: 1.01929771900177, Loss G: 0.7629004716873169\n",
            "Epoch [8/10] Batch 700/1583 Loss D: 0.14211657643318176, Loss G: 3.0680341720581055\n",
            "Epoch [8/10] Batch 800/1583 Loss D: 0.16503292322158813, Loss G: 4.558956146240234\n",
            "Epoch [8/10] Batch 900/1583 Loss D: 2.380857229232788, Loss G: 0.4558029770851135\n",
            "Epoch [8/10] Batch 1000/1583 Loss D: 0.6546363234519958, Loss G: 4.172488212585449\n",
            "Epoch [8/10] Batch 1100/1583 Loss D: 0.21659867465496063, Loss G: 3.688736915588379\n",
            "Epoch [8/10] Batch 1200/1583 Loss D: 0.1714855283498764, Loss G: 2.4636127948760986\n",
            "Epoch [8/10] Batch 1300/1583 Loss D: 0.23871475458145142, Loss G: 3.3857836723327637\n",
            "Epoch [8/10] Batch 1400/1583 Loss D: 0.1709308624267578, Loss G: 4.102027893066406\n",
            "Epoch [8/10] Batch 1500/1583 Loss D: 0.15995648503303528, Loss G: 3.1866588592529297\n",
            "Epoch [9/10] Batch 0/1583 Loss D: 0.2047429382801056, Loss G: 3.75295352935791\n",
            "Epoch [9/10] Batch 100/1583 Loss D: 0.4168200194835663, Loss G: 3.267902374267578\n",
            "Epoch [9/10] Batch 200/1583 Loss D: 0.1349770426750183, Loss G: 4.253005027770996\n",
            "Epoch [9/10] Batch 300/1583 Loss D: 0.12325449287891388, Loss G: 5.158281326293945\n",
            "Epoch [9/10] Batch 400/1583 Loss D: 0.3212023079395294, Loss G: 2.8692564964294434\n",
            "Epoch [9/10] Batch 500/1583 Loss D: 1.1132352352142334, Loss G: 1.9219951629638672\n",
            "Epoch [9/10] Batch 600/1583 Loss D: 0.20058558881282806, Loss G: 5.024259567260742\n",
            "Epoch [9/10] Batch 700/1583 Loss D: 0.154682919383049, Loss G: 4.998625755310059\n",
            "Epoch [9/10] Batch 800/1583 Loss D: 0.14875349402427673, Loss G: 2.8215200901031494\n",
            "Epoch [9/10] Batch 900/1583 Loss D: 0.1725798398256302, Loss G: 4.003713130950928\n",
            "Epoch [9/10] Batch 1000/1583 Loss D: 0.15138794481754303, Loss G: 3.3037829399108887\n",
            "Epoch [9/10] Batch 1100/1583 Loss D: 0.14501738548278809, Loss G: 4.519082546234131\n",
            "Epoch [9/10] Batch 1200/1583 Loss D: 0.16432398557662964, Loss G: 3.9412684440612793\n",
            "Epoch [9/10] Batch 1300/1583 Loss D: 0.14597418904304504, Loss G: 6.041191101074219\n",
            "Epoch [9/10] Batch 1400/1583 Loss D: 0.1608978807926178, Loss G: 4.179863929748535\n",
            "Epoch [9/10] Batch 1500/1583 Loss D: 0.10019776970148087, Loss G: 3.880830764770508\n",
            "Training Complete!\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "for epoch in range(epochs):\n",
        "    for i, data in enumerate(dataloader):\n",
        "        real_images = data[0].to(device)\n",
        "        batch_size = real_images.size(0)\n",
        "        real_labels = torch.ones(batch_size, 1, device=device)\n",
        "        fake_labels = torch.zeros(batch_size, 1, device=device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        discriminator.zero_grad()\n",
        "        output_real = discriminator(real_images).view(-1, 1)\n",
        "        loss_real = criterion(output_real, real_labels)\n",
        "\n",
        "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "        fake_images = generator(noise)\n",
        "        output_fake = discriminator(fake_images.detach()).view(-1, 1)\n",
        "        loss_fake = criterion(output_fake, fake_labels)\n",
        "\n",
        "        loss_D = loss_real + loss_fake\n",
        "        loss_D.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "        # Train Generator\n",
        "        generator.zero_grad()\n",
        "        output_fake = discriminator(fake_images).view(-1, 1)\n",
        "        loss_G = criterion(output_fake, real_labels)\n",
        "        loss_G.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch [{epoch}/{epochs}] Batch {i}/{len(dataloader)} Loss D: {loss_D.item()}, Loss G: {loss_G.item()}\")\n",
        "\n",
        "    # Save sample images after each epoch\n",
        "    with torch.no_grad():\n",
        "        fake_images = generator(torch.randn(64, nz, 1, 1, device=device)).cpu()\n",
        "    vutils.save_image(fake_images, f\"./generated_epoch_{epoch}.png\", normalize=True)\n",
        "\n",
        "print(\"Training Complete!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}